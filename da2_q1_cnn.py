# -*- coding: utf-8 -*-
"""da2_Q1_cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19u25h5N9tcF9ZiKSg0WSyy4mwKWMOZKc
"""

# Implementation using CIFAR-10 dataset


# Import necessary libraries
import numpy as np                     # For numerical computations and array handling
import matplotlib.pyplot as plt        # For plotting graphs and visualizing images
import tensorflow as tf                # For building and training neural network models
import pandas as pd                    # For data manipulation and analysis
from tensorflow.keras.datasets import cifar10  # CIFAR-10 dataset provided by Keras
from tensorflow.keras.models import Sequential, Model  # For creating sequential and functional models
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization  # Core layers for building CNNs
from tensorflow.keras.utils import to_categorical  # For one-hot encoding of labels
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # Callbacks to manage training
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For augmenting image data
from sklearn.model_selection import train_test_split  # To split the dataset into training and validation sets
from sklearn.metrics import classification_report, confusion_matrix  # For evaluating model performance
import seaborn as sns                 # For enhanced visualizations (e.g., heatmaps)
import time                           # To track the training duration


# Set random seeds for reproducibility

np.random.seed(42)                    # Ensure reproducible results for numpy operations
tf.random.set_seed(42)                # Ensure reproducibility for TensorFlow operations

# 1.1 DATASET DESCRIPTION


# Load CIFAR-10 dataset: the dataset is divided into training and test sets
(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()

# Split the full training set into a new training set and a validation set (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)

# Define class names corresponding to the 10 CIFAR-10 classes
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Print dataset information such as shapes and number of classes
print(f"Training set shape: {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")
print(f"Test set shape: {X_test.shape}")
print(f"Number of classes: {len(class_names)}")

# Visualize some sample images from the training set along with their labels
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i+1)                  # Create a 5x5 grid of subplots
    plt.imshow(X_train[i])                  # Display the image
    plt.title(class_names[y_train[i][0]])   # Set the title as the corresponding class label
    plt.axis('off')                         # Remove axis ticks for clarity
plt.tight_layout()
plt.savefig('sample_images.png')            # Save the figure for later reference
plt.show()

# Check class distribution to see if the dataset is balanced

train_counts = np.bincount(y_train.flatten())   # Count samples per class in training set
val_counts = np.bincount(y_val.flatten())         # Count samples per class in validation set
test_counts = np.bincount(y_test.flatten())       # Count samples per class in test set

plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.bar(class_names, train_counts)
plt.title('Training Class Distribution')
plt.xticks(rotation=90)                           # Rotate labels for readability

plt.subplot(1, 3, 2)
plt.bar(class_names, val_counts)
plt.title('Validation Class Distribution')
plt.xticks(rotation=90)

plt.subplot(1, 3, 3)
plt.bar(class_names, test_counts)
plt.title('Test Class Distribution')
plt.xticks(rotation=90)

plt.tight_layout()
plt.savefig('class_distribution.png')
plt.show()


# 1.2 DATA PREPROCESSING


# Function to check for corrupted images in the dataset
def check_corrupted_images(images):
    corrupted_count = 0
    for i, img in enumerate(images):
        # Check if image has constant pixel values or any NaN values (indicating corruption)
        if img.min() == img.max() or np.isnan(img).any():
            corrupted_count += 1
            print(f"Found corrupted image at index {i}")
    return corrupted_count

# Check for corrupted images in training, validation, and test sets
print(f"Corrupted images in training set: {check_corrupted_images(X_train)}")
print(f"Corrupted images in validation set: {check_corrupted_images(X_val)}")
print(f"Corrupted images in test set: {check_corrupted_images(X_test)}")

# Normalize the pixel values from [0, 255] to [0, 1] for faster convergence in training
X_train = X_train.astype('float32') / 255.0
X_val = X_val.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert integer labels to one-hot encoded vectors (for categorical cross-entropy loss)
y_train_onehot = to_categorical(y_train, 10)
y_val_onehot = to_categorical(y_val, 10)
y_test_onehot = to_categorical(y_test, 10)

# Create a data augmentation generator to enrich the training dataset with augmented images
datagen = ImageDataGenerator(
    rotation_range=15,         # Randomly rotate images by up to 15 degrees
    width_shift_range=0.1,     # Randomly shift images horizontally by up to 10% of the width
    height_shift_range=0.1,    # Randomly shift images vertically by up to 10% of the height
    horizontal_flip=True,      # Randomly flip images horizontally
    zoom_range=0.1             # Randomly zoom images in/out by up to 10%
)

# Visualize some augmented images generated from the first training sample
plt.figure(figsize=(12, 8))
for i in range(9):
    ax = plt.subplot(3, 3, i+1)
    batch = datagen.flow(X_train[0:1], batch_size=1)  # Generate one augmented image at a time
    aug_img = next(batch)[0]                           # Retrieve the generated image from the batch
    plt.imshow(aug_img)
    plt.title(class_names[y_train[0][0]])
    plt.axis('off')
plt.tight_layout()
plt.savefig('augmented_images.png')
plt.show()

# Calculate and print the mean and standard deviation of the training set for further normalization analysis
train_mean = X_train.mean()
train_std = X_train.std()
print(f"Dataset mean: {train_mean:.4f}, std: {train_std:.4f}")

# 1.3 CNN ARCHITECTURE DESIGN


# Function to create the baseline CNN model architecture
def create_baseline_cnn():
    model = Sequential([
        # First convolutional block
        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),                        # Normalizes activations and helps with training stability
        Conv2D(32, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),                # Reduces spatial dimensions by pooling
        Dropout(0.25),                               # Prevents overfitting by randomly dropping neurons

        # Second convolutional block
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Third convolutional block
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Fully connected (dense) layers for classification
        Flatten(),                                   # Flattens the 3D output to 1D for the dense layers
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(10, activation='softmax')               # Final layer with 10 units for 10 classes, softmax for probabilities
    ])

    # Compile the model with the Adam optimizer and categorical crossentropy loss
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Create the baseline CNN model instance
baseline_model = create_baseline_cnn()
baseline_model.summary()  # Print the model architecture summary

# Optionally, save a diagram of the model architecture to an image file
try:
    from tensorflow.keras.utils import plot_model
    plot_model(baseline_model, to_file='baseline_model.png', show_shapes=True, show_layer_names=True)
except:
    print("Unable to create model diagram. Make sure pydot and graphviz are installed.")

# 1.4 MODEL TRAINING AND EVALUATION


# Set up callbacks to monitor training and adjust learning parameters:
callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),  # Stop training if validation loss doesn't improve for 10 epochs
    ModelCheckpoint('best_cnn_model.h5', monitor='val_accuracy', save_best_only=True),  # Save the best model based on validation accuracy
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)   # Reduce learning rate if validation loss plateaus
]

# Record the start time for training duration measurement
start_time = time.time()

# Train the baseline model using the augmented training data
history = baseline_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=64),  # Use the data generator for augmentation
    epochs=30,                                             # Set maximum epochs to 30
    validation_data=(X_val, y_val_onehot),                 # Provide validation data for monitoring performance
    callbacks=callbacks,                                   # Include callbacks defined earlier
    verbose=1                                              # Display training progress
)

# Calculate and print the total training time
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Plot training and validation loss & accuracy over epochs
plt.figure(figsize=(12, 5))

# Plot Loss over epochs
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy over epochs
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('training_history.png')
plt.show()

# Evaluate the trained model on the test set and print loss and accuracy
test_loss, test_acc = baseline_model.evaluate(X_test, y_test_onehot)
print(f"Test accuracy: {test_acc:.4f}")
print(f"Test loss: {test_loss:.4f}")

# Generate predictions on the test set
y_pred = baseline_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)    # Convert softmax probabilities to predicted class indices
y_test_classes = np.argmax(y_test_onehot, axis=1)  # Convert one-hot encoded labels back to class indices

# Generate a detailed classification report including precision, recall, and F1-score for each class
report = classification_report(
    y_test_classes,
    y_pred_classes,
    target_names=class_names,
    output_dict=True
)

# Print the classification report to the console
print(classification_report(y_test_classes, y_pred_classes, target_names=class_names))

# Create a bar plot to visualize precision, recall, and F1-score for each class
metrics_df = pd.DataFrame(report).transpose().iloc[:-3]  # Exclude overall metrics (accuracy, macro avg, weighted avg)
plt.figure(figsize=(12, 6))
metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar')
plt.title('Performance Metrics by Class')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('performance_metrics.png')
plt.show()

# Generate and plot the confusion matrix to see the distribution of true vs. predicted labels
cm = confusion_matrix(y_test_classes, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

# Display a grid of sample test images with their true and predicted labels
plt.figure(figsize=(12, 12))
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(X_test[i])
    true_label = class_names[y_test_classes[i]]
    pred_label = class_names[y_pred_classes[i]]
    # Color code the title: green if prediction is correct, red if incorrect
    color = 'green' if true_label == pred_label else 'red'
    plt.title(f"T: {true_label}\nP: {pred_label}", color=color)
    plt.axis('off')
plt.tight_layout()
plt.savefig('sample_predictions.png')
plt.show()


# 1.5 MODEL IMPROVEMENT
# In this section, two improved architectures are defined: a deeper CNN and a ResNet-like CNN.

# Function to create a deeper CNN variant with additional convolutional layers
def create_deeper_cnn():
    model = Sequential([
        # First convolutional block with an extra Conv2D layer
        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        Conv2D(32, (3, 3), padding='same', activation='relu'),
        Conv2D(32, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Second convolutional block with an extra Conv2D layer
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Third convolutional block with an extra Conv2D layer
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),

        # Fully connected layers for classification
        Flatten(),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(10, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Function to create a CNN model with residual connections (similar to ResNet architecture)
def create_resnet_like_cnn():
    # Input layer for the model
    inputs = Input(shape=(32, 32, 3))

    # First convolutional block with residual connection
    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Conv2D(32, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    # Create a residual connection from the input
    residual = Conv2D(32, (1, 1), padding='same')(inputs)
    x = tf.keras.layers.add([x, residual])   # Element-wise addition for the residual connection
    x = tf.keras.layers.Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.25)(x)

    # Second convolutional block with residual connection
    prev_x = x                                # Save previous output for the residual connection
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(64, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    residual = Conv2D(64, (1, 1), padding='same')(prev_x)
    x = tf.keras.layers.add([x, residual])
    x = tf.keras.layers.Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.25)(x)

    # Third convolutional block with residual connection
    prev_x = x
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    residual = Conv2D(128, (1, 1), padding='same')(prev_x)
    x = tf.keras.layers.add([x, residual])
    x = tf.keras.layers.Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.25)(x)

    # Fully connected layers for final classification
    x = Flatten()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    outputs = Dense(10, activation='softmax')(x)

    # Define the model using the functional API
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Create model variations: deeper CNN and ResNet-like CNN
deeper_model = create_deeper_cnn()
resnet_model = create_resnet_like_cnn()

# Print model summaries to review architectures
deeper_model.summary()
resnet_model.summary()


# Training and Evaluation of Model Variants

# Store all models in a dictionary for easy iteration
models = {
    'baseline': baseline_model,
    'deeper': deeper_model,
    'resnet_like': resnet_model
}

results = {}  # Dictionary to store evaluation results for each model

# Loop through each model to train and evaluate
for name, model in models.items():
    if name == 'baseline':
        # For baseline, use the already trained model and its metrics
        results[name] = {
            'accuracy': test_acc,
            'history': history.history
        }
        continue  # Skip re-training the baseline model

    print(f"\n\nTraining {name} model...")
    start_time = time.time()

    # Train the model with data augmentation
    history = model.fit(
        datagen.flow(X_train, y_train_onehot, batch_size=64),
        epochs=30,
        validation_data=(X_val, y_val_onehot),
        callbacks=callbacks,
        verbose=1
    )

    training_time = time.time() - start_time
    print(f"Training completed in {training_time:.2f} seconds")

    # Evaluate the model on the test set
    test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)
    results[name] = {
        'accuracy': test_acc,
        'history': history.history,
        'training_time': training_time
    }
    print(f"{name} model test accuracy: {test_acc:.4f}")

    # Save the trained model to a file
    model.save(f'best_{name}_model.h5')


# Model Comparison: Visualize performance across models

plt.figure(figsize=(12, 5))

# Plot validation accuracy for each model over epochs
plt.subplot(1, 2, 1)
for name, result in results.items():
    plt.plot(result['history']['val_accuracy'], label=f"{name} (acc={result['accuracy']:.4f})")
plt.title('Validation Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plot validation loss for each model over epochs
plt.subplot(1, 2, 2)
for name, result in results.items():
    plt.plot(result['history']['val_loss'], label=f"{name}")
plt.title('Validation Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.show()

# Bar chart to compare the final test accuracy of each model
final_accuracies = {name: result['accuracy'] for name, result in results.items()}
plt.figure(figsize=(10, 6))
plt.bar(final_accuracies.keys(), final_accuracies.values())
plt.title('Test Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
for i, (name, acc) in enumerate(final_accuracies.items()):
    plt.text(i, acc + 0.01, f"{acc:.4f}", ha='center')
plt.tight_layout()
plt.savefig('final_accuracy_comparison.png')
plt.show()


# Detailed Evaluation for the Best Model

# Determine the best model based on test accuracy
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model = models[best_model_name]
print(f"Best model: {best_model_name} with accuracy: {results[best_model_name]['accuracy']:.4f}")

# Generate predictions with the best model on the test set
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Create and print a classification report for the best model
best_report = classification_report(
    y_test_classes,
    y_pred_classes,
    target_names=class_names,
    output_dict=True
)
print("Classification Report for Best Model:")
print(classification_report(y_test_classes, y_pred_classes, target_names=class_names))

# Save the best performing model to a file
best_model.save('final_best_model.h5')

print("Complete implementation finished!")

